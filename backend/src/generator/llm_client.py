# backend/src/llm_client.py
import os
from openai import OpenAI

# ========== –î–û–ë–ê–í–¨–¢–ï –≠–¢–ò –°–¢–†–û–ö–ò ==========
from dotenv import load_dotenv
import sys
from pathlib import Path


# –ù–∞—Ö–æ–¥–∏–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–≥–¥–µ .env —Ñ–∞–π–ª)
project_root = Path(__file__).parent.parent.parent
env_path = project_root / ".env"

print(f"üîç –ò—â—É .env —Ñ–∞–π–ª –ø–æ –ø—É—Ç–∏: {env_path}")

if env_path.exists():
    load_dotenv(env_path)
    print("‚úÖ .env —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω")
else:
    print("‚ùå .env —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print(f"   –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª: {env_path}")
    print("   –° —Å–æ–¥–µ—Ä–∂–∏–º—ã–º: API_KEY=–≤–∞—à_–∫–ª—é—á_–æ—Ç_sbercloud")
    sys.exit(1)
# ==========================================

# –¢–µ–ø–µ—Ä—å –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
api_key = os.environ.get("API_KEY")

if not api_key:
    print("‚ùå API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ .env —Ñ–∞–π–ª")
    sys.exit(1)

print(f"‚úÖ API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω: {api_key[:15]}...")

url = "https://foundation-models.api.cloud.ru/v1"

client = OpenAI(
    api_key=api_key,
    base_url=url
)

# –í–∞—à–∞ —Ñ—É–Ω–∫—Ü–∏—è call_llm –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
async def call_llm(messages, temperature=0.1, max_tokens=4000):
    try:
        response = client.chat.completions.create(
            model="GigaChat",
            max_tokens=max_tokens,
            temperature=temperature,
            presence_penalty=0,
            top_p=0.95,
            messages=messages
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ LLM: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
        return "import allure\n\n# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ä–µ–∂–∏–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)\nprint('–¢–µ—Å—Ç-–∫–µ–π—Å—ã –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–∏ —Ä–∞–±–æ—á–µ–º API –∫–ª—é—á–µ')"
